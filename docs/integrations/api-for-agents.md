# API for AI Agents

<!--
Tags for AI agents:
- api-documentation
- integration
- authentication
- endpoints
- local-development
- openapi-spec
- code-examples
- python
- javascript
- curl
- providers
- model-configuration
-->

Complete guide for AI agents and automated systems to integrate with AI Gateway. This documentation covers API endpoints, authentication, local development, and best practices.

## Quick Links

- **[Authentication](#authentication)** - Virtual Key and Master Key
- **[API Endpoints](#api-endpoints)** - Core endpoints and examples
- **[Local Development](#local-development-and-testing)** - Setup for local testing
- **[OpenAPI Specification](#getting-openapi-specification)** - Get OpenAPI spec
- **[Supported Providers](#supported-providers)** - Provider configuration
- **[Examples](../examples/README.md)** - Code examples in Python, JavaScript

**Related Documentation:**
- [Getting Started](../getting-started.md) - Initial setup
- [Configuration Guide](../configuration.md) - Configuration options
- [Troubleshooting](../troubleshooting.md) - API issues

## Overview

AI Gateway exposes a unified LiteLLM API that provides access to multiple LLM providers through a single interface. The API follows the OpenAI-compatible format, making it easy to integrate with existing tools and frameworks.

### Base URLs

**With Nginx (default):**
- API Base: `http://localhost:PORT/api/litellm/v1`
- Example: `http://localhost:3000/api/litellm/v1`

**Without Nginx:**
- API Base: `http://localhost:4000/v1`
- Example: `http://localhost:4000/v1`

**From Docker network (internal):**
- API Base: `http://litellm:4000/v1`

> **Note:** Replace `localhost:PORT` with your configured port. Check `.env` file for `NGINX_HTTP_PORT` (with Nginx) or `LITELLM_EXTERNAL_PORT` (without Nginx).

## Authentication

AI Gateway uses **Bearer token authentication** with Virtual Keys or Master Key.

### Virtual Key (Recommended)

Virtual Keys provide better security and access control. They are created in LiteLLM Admin UI and stored in `.env` file.

**Get Virtual Key:**
```bash
# From .env file
grep VIRTUAL_KEY .env
# Output: VIRTUAL_KEY=sk-xxxxxxxxxxxxx
```

**Use in requests:**
```bash
curl -H "Authorization: Bearer sk-xxxxxxxxxxxxx" \
  http://localhost:3000/api/litellm/v1/models
```

### Master Key (Not Recommended)

Master Key provides full access but is less secure. Use only for testing or when Virtual Key is not available.

**Get Master Key:**
```bash
# From .env file
grep LITELLM_MASTER_KEY .env
# Output: LITELLM_MASTER_KEY=sk-xxxxxxxxxxxxx
```

> ‚ö†Ô∏è **Security Warning:** Never expose Master Key in client applications. Always use Virtual Keys for API access.

## API Endpoints

AI Gateway supports all standard OpenAI-compatible endpoints through LiteLLM. The API follows the OpenAI format: https://platform.openai.com/docs/api-reference

### Core Endpoints

#### List Models
Get list of available models.

**Endpoint:** `GET /v1/models`

**Example:**
```bash
curl http://localhost:3000/api/litellm/v1/models \
  -H "Authorization: Bearer YOUR_VIRTUAL_KEY"
```

**Response:**
```json
{
  "data": [
    {
      "id": "claude-sonnet-4-5",
      "object": "model",
      "created": 1234567890,
      "owned_by": "anthropic"
    },
    {
      "id": "gpt-4o",
      "object": "model",
      "created": 1234567890,
      "owned_by": "openai"
    }
  ],
  "object": "list"
}
```

#### Chat Completions
Generate chat completions (most common endpoint).

**Endpoint:** `POST /v1/chat/completions`

**Example:**
```bash
curl http://localhost:3000/api/litellm/v1/chat/completions \
  -H "Authorization: Bearer YOUR_VIRTUAL_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "claude-sonnet-4-5",
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "temperature": 0.7,
    "max_tokens": 1000
  }'
```

**Response:**
```json
{
  "id": "chatcmpl-123",
  "object": "chat.completion",
  "created": 1234567890,
  "model": "claude-sonnet-4-5",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Hello! How can I help you today?"
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 10,
    "completion_tokens": 12,
    "total_tokens": 22
  }
}
```

#### Streaming Chat Completions
Get streaming responses for real-time applications.

**Endpoint:** `POST /v1/chat/completions` (with `stream: true`)

**Example:**
```bash
curl http://localhost:3000/api/litellm/v1/chat/completions \
  -H "Authorization: Bearer YOUR_VIRTUAL_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "claude-sonnet-4-5",
    "messages": [{"role": "user", "content": "Tell me a story"}],
    "stream": true
  }'
```

**Response (Server-Sent Events):**
```
data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1234567890,"model":"claude-sonnet-4-5","choices":[{"index":0,"delta":{"content":"Once"},"finish_reason":null}]}

data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1234567890,"model":"claude-sonnet-4-5","choices":[{"index":0,"delta":{"content":" upon"},"finish_reason":null}]}

data: [DONE]
```

### Additional Endpoints

AI Gateway supports all standard OpenAI endpoints:

- **Completions:** `POST /v1/completions` - Text completions
- **Embeddings:** `POST /v1/embeddings` - Generate embeddings
- **Audio:** `POST /v1/audio/transcriptions` - Transcribe audio
- **Audio:** `POST /v1/audio/translations` - Translate audio
- **Images:** `POST /v1/images/generations` - Generate images
- **Moderations:** `POST /v1/moderations` - Content moderation
- **Files:** `GET /v1/files`, `POST /v1/files`, `DELETE /v1/files/{file_id}` - File operations
- **Assistants:** `POST /v1/assistants`, `GET /v1/assistants/{assistant_id}` - Assistants API
- **Threads:** `POST /v1/threads`, `GET /v1/threads/{thread_id}` - Threads API
- **Messages:** `POST /v1/threads/{thread_id}/messages` - Messages API

> **Note:** Not all providers support all endpoints. Check provider documentation for supported features.

## Supported Providers

AI Gateway uses LiteLLM, which supports **100+ LLM providers**. This section covers the most popular providers and how to configure them.

> üìö **Complete Provider List:** See [LiteLLM Providers Documentation](https://docs.litellm.ai/docs/providers) for the full list of 100+ supported providers, setup instructions, and provider-specific details.

### Provider Configuration

Providers can be configured in two ways:

1. **Via LiteLLM Admin UI (Recommended):**
   - Open http://localhost:4000/ui
   - Go to "Providers" section
   - Add provider and API key
   - See [Getting Started Guide](../getting-started.md#step-32-add-providers) for details

2. **Via Environment Variables:**
   - Add API key to `.env` file
   - Format: `{PROVIDER}_API_KEY=your-key-here`
   - Example: `ANTHROPIC_API_KEY=sk-ant-...`

### Popular Providers

#### Anthropic (Claude)
- **Environment Variable:** `ANTHROPIC_API_KEY`
- **Get API Key:** https://console.anthropic.com/
- **LiteLLM Docs:** https://docs.litellm.ai/docs/providers/anthropic
- **Model IDs:** Check LiteLLM docs or `/v1/models` endpoint for exact IDs
  - Common patterns: `claude-3-5-sonnet`, `claude-3-opus`, `claude-3-haiku`, `claude-sonnet-4-5`
  - ‚ö†Ô∏è **Note:** Model IDs may differ from official names - always verify via API
- ‚ö†Ô∏è **Note:** Tier 1 has strict rate limits - Tier 2+ recommended

#### OpenAI (GPT)
- **Environment Variable:** `OPENAI_API_KEY`
- **Get API Key:** https://platform.openai.com/api-keys
- **Models:** `gpt-4o`, `gpt-4o-mini`, `gpt-4-turbo`, `gpt-3.5-turbo`
- **Docs:** https://docs.litellm.ai/docs/providers/openai

#### Google AI Studio (Gemini)
- **Environment Variable:** `GEMINI_API_KEY` or `GOOGLE_API_KEY`
- **Get API Key:** https://makersuite.google.com/app/apikey
- **Models:** `gemini-pro`, `gemini-pro-vision`
- **Docs:** https://docs.litellm.ai/docs/providers/google_ai_studio

#### Azure OpenAI
- **Environment Variables:** `AZURE_API_KEY`, `AZURE_API_BASE`, `AZURE_API_VERSION`
- **Get API Key:** Azure Portal
- **Models:** Same as OpenAI (deployed on Azure)
- **Docs:** https://docs.litellm.ai/docs/providers/azure_openai

#### Groq
- **Environment Variable:** `GROQ_API_KEY`
- **Get API Key:** https://console.groq.com/keys
- **Models:** Fast inference for open models
- **Docs:** https://docs.litellm.ai/docs/providers/groq

#### Mistral AI
- **Environment Variable:** `MISTRAL_API_KEY`
- **Get API Key:** https://console.mistral.ai/
- **Models:** `mistral-large`, `mistral-medium`, `mistral-small`
- **Docs:** https://docs.litellm.ai/docs/providers/mistral

#### Deepseek
- **Environment Variable:** `DEEPSEEK_API_KEY`
- **Get API Key:** https://deepseek.com/
- **Models:** `deepseek-chat`, `deepseek-coder`
- **Docs:** https://docs.litellm.ai/docs/providers/deepseek

#### Together AI
- **Environment Variable:** `TOGETHER_API_KEY`
- **Get API Key:** https://together.ai/
- **Models:** Various open models
- **Docs:** https://docs.litellm.ai/docs/providers/together

#### Perplexity AI
- **Environment Variable:** `PERPLEXITY_API_KEY`
- **Get API Key:** https://www.perplexity.ai
- **Models:** `pplx-70b-online`, `pplx-7b-online`
- **Docs:** https://docs.litellm.ai/docs/providers/perplexity

#### xAI (Grok)
- **Environment Variable:** `XAI_API_KEY`
- **Get API Key:** https://docs.x.ai/docs
- **Models:** `grok-beta`
- **Docs:** https://docs.litellm.ai/docs/providers/xai

#### OpenRouter
- **Environment Variable:** `OPENROUTER_API_KEY`
- **Get API Key:** https://openrouter.ai/
- **Models:** Access to multiple providers through one API
- **Docs:** https://docs.litellm.ai/docs/providers/openrouter

#### Local Models (No API Key Required)

**Ollama:**
- **Setup:** Local deployment, no API key
- **Models:** Any model supported by Ollama
- **Docs:** https://docs.litellm.ai/docs/providers/ollama

**LM Studio:**
- **Setup:** Local deployment, no API key
- **Models:** Any model supported by LM Studio
- **Docs:** https://docs.litellm.ai/docs/providers/lm_studio

### Checking Available Models

> ‚ö†Ô∏è **IMPORTANT:** Model IDs in LiteLLM may differ from provider's official names and can be customized. **Always check actual model IDs** using the methods below, as they may not match examples in documentation.

After configuring providers, check available models:

```bash
# List all available models (recommended method)
curl http://localhost:3000/api/litellm/v1/models \
  -H "Authorization: Bearer YOUR_VIRTUAL_KEY" | jq '.data[].id'

# Or format as table
curl http://localhost:3000/api/litellm/v1/models \
  -H "Authorization: Bearer YOUR_VIRTUAL_KEY" | jq -r '.data[] | "\(.id) - \(.owned_by)"'
```

Or via Python:
```python
import requests

response = requests.get(
    "http://localhost:3000/api/litellm/v1/models",
    headers={"Authorization": f"Bearer {API_KEY}"}
)
models = response.json()['data']
for model in models:
    print(f"{model['id']} - {model.get('owned_by', 'unknown')}")
```

**Alternative Methods:**
- **LiteLLM Admin UI:** Go to Models section to see all configured models
- **Provider Documentation:** Check [LiteLLM Providers Docs](https://docs.litellm.ai/docs/providers) for each provider's specific model ID formats

### Provider-Specific Notes

- **Rate Limits:** Each provider has different rate limits. Check provider documentation.
- **Model IDs:** ‚ö†Ô∏è **CRITICAL** - Model IDs may vary by provider and can differ from official names. 
  - Always check `/v1/models` endpoint for actual IDs in your setup
  - Model IDs can be customized in LiteLLM Admin UI
  - Check [LiteLLM Providers Docs](https://docs.litellm.ai/docs/providers) for provider-specific formats
- **Costs:** Different providers have different pricing. Check provider pricing pages.
- **Features:** Not all providers support all features (e.g., streaming, function calling).

> **For complete provider information:** See [LiteLLM Providers Documentation](https://docs.litellm.ai/docs/providers) for:
> - Full list of 100+ providers
> - Provider-specific setup instructions
> - Environment variable names
> - Model ID formats
> - Rate limits and pricing

## Code Examples

### Python

```python
import requests
import os

# Configuration
API_BASE = "http://localhost:3000/api/litellm/v1"
API_KEY = os.getenv("VIRTUAL_KEY", "your-virtual-key-here")

# List models
response = requests.get(
    f"{API_BASE}/models",
    headers={"Authorization": f"Bearer {API_KEY}"}
)
models = response.json()
print(f"Available models: {[m['id'] for m in models['data']]}")

# Chat completion
response = requests.post(
    f"{API_BASE}/chat/completions",
    headers={
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json"
    },
    json={
        "model": "claude-sonnet-4-5",
        "messages": [
            {"role": "user", "content": "Hello!"}
        ],
        "temperature": 0.7,
        "max_tokens": 1000
    }
)
result = response.json()
print(f"Response: {result['choices'][0]['message']['content']}")
```

### JavaScript/TypeScript

```javascript
const API_BASE = "http://localhost:3000/api/litellm/v1";
const API_KEY = process.env.VIRTUAL_KEY || "your-virtual-key-here";

// List models
async function listModels() {
  const response = await fetch(`${API_BASE}/models`, {
    headers: {
      "Authorization": `Bearer ${API_KEY}`
    }
  });
  const data = await response.json();
  console.log("Available models:", data.data.map(m => m.id));
}

// Chat completion
async function chatCompletion(message) {
  const response = await fetch(`${API_BASE}/chat/completions`, {
    method: "POST",
    headers: {
      "Authorization": `Bearer ${API_KEY}`,
      "Content-Type": "application/json"
    },
    body: JSON.stringify({
      model: "claude-sonnet-4-5",
      messages: [
        { role: "user", content: message }
      ],
      temperature: 0.7,
      max_tokens: 1000
    })
  });
  const result = await response.json();
  return result.choices[0].message.content;
}
```

### Using OpenAI SDK (Compatible)

Since AI Gateway uses OpenAI-compatible API, you can use OpenAI SDKs:

**Python:**
```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:3000/api/litellm/v1",
    api_key="your-virtual-key-here"
)

response = client.chat.completions.create(
    model="claude-sonnet-4-5",
    messages=[{"role": "user", "content": "Hello!"}]
)
print(response.choices[0].message.content)
```

**JavaScript:**
```javascript
import OpenAI from 'openai';

const client = new OpenAI({
  baseURL: 'http://localhost:3000/api/litellm/v1',
  apiKey: 'your-virtual-key-here'
});

const response = await client.chat.completions.create({
  model: 'claude-sonnet-4-5',
  messages: [{ role: 'user', content: 'Hello!' }]
});
console.log(response.choices[0].message.content);
```

## Local Development and Testing

AI agents can set up and test AI Gateway locally for development and integration testing.

### Prerequisites

- **Docker**: 20.10+ (Docker Compose v2)
- **Python**: 3.8+ (for setup scripts)
- **RAM**: 3GB minimum (4GB recommended)
- **Disk**: 10GB minimum

### Step 1: Clone and Setup

```bash
# Clone repository
git clone <repository-url>
cd ai-gateway

# Run setup
./setup.sh  # Linux/macOS
# or
setup.bat   # Windows
```

**Setup will:**
- Configure resource profile
- Set up ports
- Generate `.env` file
- Create `config.yaml`
- Generate `docker-compose.override.yml`

### Step 2: Configure Environment

**Minimal `.env` for local testing:** ‚úÖ **–ü–†–û–í–ï–†–ï–ù–û**

> ‚úÖ **–ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ:** –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–æ–≤–µ—Ä–µ–Ω –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–º —Å–µ—Ä–≤–µ—Ä–µ (2025-11-26)

```bash
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–´–ï –ü–ï–†–ï–ú–ï–ù–ù–´–ï (–¥–ª—è –∑–∞–ø—É—Å–∫–∞ setup –∏ start)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# Master Key (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ, –¥–æ–ª–∂–µ–Ω –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å "sk-")
# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è: openssl rand -base64 32
LITELLM_MASTER_KEY=sk-your-master-key-here

# PostgreSQL (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ)
POSTGRES_USER=litellm
POSTGRES_PASSWORD=litellm_password  # –ú–∏–Ω–∏–º—É–º 8 —Å–∏–º–≤–æ–ª–æ–≤
POSTGRES_DB=litellm
POSTGRES_PORT=5432

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –û–ü–¶–ò–û–ù–ê–õ–¨–ù–´–ï –ü–ï–†–ï–ú–ï–ù–ù–´–ï (–º–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å –ø—É—Å—Ç—ã–º–∏)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# UI Credentials (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: admin/change_this_password_123)
UI_USERNAME=admin
UI_PASSWORD=change_this_password_123  # –ú–∏–Ω–∏–º—É–º 8 —Å–∏–º–≤–æ–ª–æ–≤, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω

# WebUI Secret Key (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –º–∏–Ω–∏–º—É–º 16 —Å–∏–º–≤–æ–ª–æ–≤, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω)
WEBUI_SECRET_KEY=

# API Keys –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ (–ù–ï –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã –¥–ª—è –∑–∞–ø—É—Å–∫–∞!)
# –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø–æ–∑–∂–µ —á–µ—Ä–µ–∑ LiteLLM Admin UI
# ANTHROPIC_API_KEY=your-key-here
# OPENAI_API_KEY=your-key-here
# GEMINI_API_KEY=your-key-here
# GROQ_API_KEY=your-key-here

# Virtual Key (—Å–æ–∑–¥–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ –∑–∞–ø—É—Å–∫–∞)
# VIRTUAL_KEY=sk-xxxxxxxxxxxxx

# –ü–æ—Ä—Ç—ã (–Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏ setup)
# LITELLM_EXTERNAL_PORT=4000
# NGINX_HTTP_PORT=63345
# USE_NGINX=yes
```

**–í–∞–∂–Ω–æ:**
- ‚úÖ API –∫–ª—é—á–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ **–ù–ï –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã** –¥–ª—è –∑–∞–ø—É—Å–∫–∞ setup/start
- ‚úÖ –ú–æ–∂–Ω–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å —Å–∏—Å—Ç–µ–º—É –±–µ–∑ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ –∏ –¥–æ–±–∞–≤–∏—Ç—å –∏—Ö –ø–æ–∑–∂–µ —á–µ—Ä–µ–∑ LiteLLM Admin UI
- ‚úÖ PostgreSQL **–æ–±—è–∑–∞—Ç–µ–ª–µ–Ω** - —Å–∏—Å—Ç–µ–º–∞ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ –Ω–µ–≥–æ (in-memory –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è)
- ‚úÖ –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è: 3GB RAM, Docker 20.10+, Python 3.8+

**Isolation from other projects:** ‚úÖ **–ü–†–û–í–ï–†–ï–ù–û**

> ‚úÖ **–ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ:** –ò–∑–æ–ª—è—Ü–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —á–µ—Ä–µ–∑ Docker Compose (–ø—Ä–æ–≤–µ—Ä–µ–Ω–æ –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ 2025-11-26)

1. **Docker network (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∏–∑–æ–ª—è—Ü–∏—è):**
   - ‚úÖ AI Gateway —Å–æ–∑–¥–∞–µ—Ç –æ—Ç–¥–µ–ª—å–Ω—É—é Docker network –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
   - ‚úÖ –ò–º—è —Å–µ—Ç–∏: `{project}_litellm-network` (–Ω–∞–ø—Ä–∏–º–µ—Ä, `ai-gateway_litellm-network`)
   - ‚úÖ –¢–∏–ø: `bridge` - –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–µ—Ç—å –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞
   - ‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞: `docker network ls | grep litellm`

2. **–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏–º–µ–Ω–∞ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤:**
   - ‚úÖ –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–º–µ–Ω–∞: `litellm-proxy`, `litellm-postgres`, `open-webui`, `litellm-nginx`
   - ‚úÖ –ò–º–µ–Ω–∞ —É–Ω–∏–∫–∞–ª—å–Ω—ã –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞ (Docker Compose –¥–æ–±–∞–≤–ª—è–µ—Ç –ø—Ä–µ—Ñ–∏–∫—Å –ø—Ä–æ–µ–∫—Ç–∞)
   - ‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞: `docker ps --format "table {{.Names}}\t{{.Image}}"`

3. **–ò–∑–±–µ–∂–∞–Ω–∏–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ –ø–æ—Ä—Ç–æ–≤:**
   - ‚úÖ –í–æ –≤—Ä–µ–º—è setup –º–æ–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å "random high ports"
   - ‚úÖ –ü–æ—Ä—Ç—ã –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ 49152-65535
   - ‚úÖ –ü–æ—Ä—Ç—ã –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—é—Ç—Å—è –≤ `.env` –∏ `docker-compose.override.yml`
   - ‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞: `docker compose ps` –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –º–∞–ø–ø–∏–Ω–≥ –ø–æ—Ä—Ç–æ–≤

4. **–û—Ç–¥–µ–ª—å–Ω—ã–π `.env` —Ñ–∞–π–ª:**
   - ‚úÖ `.env` —Å–ø–µ—Ü–∏—Ñ–∏—á–µ–Ω –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞ (–Ω–µ –∫–æ–º–º–∏—Ç–∏—Ç—Å—è –≤ git)
   - ‚úÖ –ö–∞–∂–¥—ã–π –ø—Ä–æ–µ–∫—Ç –∏–º–µ–µ—Ç —Å–≤–æ—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
   - ‚úÖ –ü—Ä–∞–≤–∞ –¥–æ—Å—Ç—É–ø–∞: 600 (—Ç–æ–ª—å–∫–æ –≤–ª–∞–¥–µ–ª–µ—Ü)

### Step 3: Start Locally

```bash
# Start containers
./start.sh  # Linux/macOS
# or
start.bat   # Windows
```

**Wait for containers to be healthy:**
- Setup waits for health checks automatically
- Containers are ready when you see: "‚úÖ Containers started and healthy!"

### Step 4: Create Virtual Key

```bash
# Create Virtual Key
./virtual-key.sh  # Linux/macOS
# or
virtual-key.bat   # Windows
```

Virtual Key will be saved to `.env` file automatically.

### Step 5: Test API

```bash
# Test API endpoint
curl http://localhost:3000/api/litellm/v1/models \
  -H "Authorization: Bearer YOUR_VIRTUAL_KEY"

# Or if using Master Key (not recommended)
curl http://localhost:3000/api/litellm/v1/models \
  -H "Authorization: Bearer YOUR_MASTER_KEY"
```

### Step 6: Verify System Health ‚úÖ **–ß–ï–ö–õ–ò–°–¢ –ü–†–û–í–ï–†–ö–ò**

**–ü–æ—Å–ª–µ setup:**
```bash
# 1. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–æ–≤
ls -la .env config.yaml docker-compose.override.yml

# 2. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø—Ä–∞–≤–∞ –¥–æ—Å—Ç—É–ø–∞ (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å 600)
stat -c "%a %n" .env config.yaml docker-compose.override.yml

# 3. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –≤ .env
grep -E "^LITELLM_MASTER_KEY=|^POSTGRES_" .env
```

**–ü–æ—Å–ª–µ start:**
```bash
# 1. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ç–∞—Ç—É—Å –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤ (–≤—Å–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å "Up" –∏ "healthy")
docker compose ps

# 2. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å health checks
docker compose ps --format json | python3 -m json.tool | grep -A 2 Health
# –û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: "healthy" –¥–ª—è postgres, litellm, open-webui

# 3. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å API –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å (—á–µ—Ä–µ–∑ Nginx)
curl -s http://localhost:PORT/api/litellm/v1/models \
  -H "Authorization: Bearer YOUR_VIRTUAL_KEY" | python3 -m json.tool | head -20

# 4. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å API –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å (–Ω–∞–ø—Ä—è–º—É—é –∫ LiteLLM, –µ—Å–ª–∏ –ø–æ—Ä—Ç exposed)
LITELLM_PORT=$(grep LITELLM_EXTERNAL_PORT .env | cut -d= -f2)
VIRTUAL_KEY=$(grep VIRTUAL_KEY .env | cut -d= -f2)
curl -s http://localhost:${LITELLM_PORT}/v1/models \
  -H "Authorization: Bearer ${VIRTUAL_KEY}" | python3 -m json.tool | head -20

# 5. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å OpenAPI spec (–≤–Ω—É—Ç—Ä–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞)
docker exec litellm-proxy python3 -c "import requests; r = requests.get('http://localhost:4000/openapi.json'); print('‚úÖ OpenAPI spec –¥–æ—Å—Ç—É–ø–µ–Ω' if r.status_code == 200 else '‚ùå OpenAPI spec –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω'); print(f'–†–∞–∑–º–µ—Ä: {len(r.text)} –±–∞–π—Ç'); data=r.json() if r.status_code == 200 else {}; print(f'Endpoints: {len(data.get(\"paths\", {}))}') if data else None"
# –û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: ‚úÖ OpenAPI spec –¥–æ—Å—Ç—É–ø–µ–Ω, –†–∞–∑–º–µ—Ä: ~776KB, Endpoints: 330

# 6. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å OpenAPI spec (—á–µ—Ä–µ–∑ –≤–Ω–µ—à–Ω–∏–π –ø–æ—Ä—Ç —Å Master Key)
LITELLM_PORT=$(grep LITELLM_EXTERNAL_PORT .env | cut -d= -f2)
MASTER_KEY=$(grep LITELLM_MASTER_KEY .env | cut -d= -f2)
curl -s http://localhost:${LITELLM_PORT}/openapi.json \
  -H "Authorization: Bearer ${MASTER_KEY}" | python3 -c "import sys, json; data=json.load(sys.stdin); print(f'OpenAPI {data.get(\"openapi\")}, {len(data.get(\"paths\", {}))} endpoints')"
# –û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: OpenAPI 3.1.0, 330 endpoints

# 7. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å health check
docker exec litellm-proxy python3 -c "import requests; r = requests.get('http://localhost:4000/health/liveliness'); print('‚úÖ Health check OK' if r.status_code == 200 else f'‚ùå Health check failed: {r.status_code}'); print(f'Response: {r.text[:50]}')"
# –û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: ‚úÖ Health check OK, Response: I'm alive!

# 8. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ª–æ–≥–∏ –Ω–∞ –æ—à–∏–±–∫–∏
docker compose logs | grep -i error | tail -20

# 9. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å Docker network
docker network ls | grep litellm
docker network inspect ai-gateway_litellm-network 2>/dev/null | python3 -c "import sys, json; data=json.load(sys.stdin); print(f'Network: {data[0][\"Name\"]}'); print(f'Containers: {len(data[0][\"Containers\"])}')"
# –û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: Network: ai-gateway_litellm-network, Containers: 4
```

### Troubleshooting Local Setup

**Port conflicts:**
```bash
# Check what's using the port
sudo lsof -i :3000  # Linux/macOS
netstat -ano | findstr :3000  # Windows

# Solution: Re-run setup and choose different ports
./setup.sh
```

**Containers won't start:**
```bash
# Check container status
docker compose ps

# View logs
docker compose logs

# Restart
./stop.sh
./start.sh
```

**Virtual Key creation fails:**
- Wait 45+ seconds after container start (LiteLLM needs time to initialize)
- Check LiteLLM logs: `docker compose logs litellm-proxy`
- Create manually via LiteLLM Admin UI: http://localhost:4000/ui

## Getting OpenAPI Specification

OpenAPI specification provides complete API documentation in machine-readable format. This is useful for:
- Understanding all available endpoints
- Generating API clients automatically
- Validating requests/responses
- API documentation tools

### Method 1: From Running Container ‚úÖ **–ü–†–û–í–ï–†–ï–ù–û**

**Get OpenAPI spec from LiteLLM container:**

> ‚úÖ **–ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ:** OpenAPI spec –¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ LiteLLM (–ø—Ä–æ–≤–µ—Ä–µ–Ω–æ –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ 2025-11-26)

```bash
# Method 1: Via HTTP endpoint using Python (curl –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ)
docker exec litellm-proxy python3 -c "import requests; r = requests.get('http://localhost:4000/openapi.json'); print(r.text if r.status_code == 200 else f'Error: {r.status_code}')" > openapi.json

# Method 2: Via wget (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
docker exec litellm-proxy wget -qO- http://localhost:4000/openapi.json > openapi.json

# Method 3: Check OpenAPI module and Swagger UI files
docker exec litellm-proxy find /app -name "*openapi*" -o -name "*swagger*"
# –†–µ–∑—É–ª—å—Ç–∞—Ç: /app/litellm/proxy/swagger/, /app/litellm/proxy/common_utils/custom_openapi_spec.py

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏
docker exec litellm-proxy python3 -c "import requests; r = requests.get('http://localhost:4000/openapi.json'); print(f'Status: {r.status_code}, Size: {len(r.text)} bytes, Content-Type: {r.headers.get(\"Content-Type\")}')"
# –û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: Status: 200, Size: ~776KB, Content-Type: application/json
```

**–§–æ—Ä–º–∞—Ç:** OpenAPI 3.1.0, JSON, —Ä–∞–∑–º–µ—Ä ~776KB, **330 endpoints** ‚úÖ **–ü–†–û–í–ï–†–ï–ù–û**

> ‚úÖ **–ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ (2025-11-26):**
> - OpenAPI version: 3.1.0
> - Info title: LiteLLM API
> - Paths count: 330 endpoints
> - –î–æ—Å—Ç—É–ø–µ–Ω —á–µ—Ä–µ–∑: `http://localhost:4000/openapi.json` (–≤–Ω—É—Ç—Ä–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞) –∏–ª–∏ —á–µ—Ä–µ–∑ –≤–Ω–µ—à–Ω–∏–π –ø–æ—Ä—Ç —Å –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–µ–π

### Method 2: From Host (if port is exposed) ‚úÖ **–ü–†–û–í–ï–†–ï–ù–û**

> ‚úÖ **–ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ:** OpenAPI spec –¥–æ—Å—Ç—É–ø–µ–Ω —á–µ—Ä–µ–∑ –≤–Ω–µ—à–Ω–∏–π –ø–æ—Ä—Ç —Å Master Key (–ø—Ä–æ–≤–µ—Ä–µ–Ω–æ –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ 2025-11-26)

```bash
# If LiteLLM port is exposed (without Nginx or via separate port)
# Get port from .env: LITELLM_EXTERNAL_PORT
LITELLM_PORT=$(grep LITELLM_EXTERNAL_PORT .env | cut -d= -f2)
MASTER_KEY=$(grep LITELLM_MASTER_KEY .env | cut -d= -f2)

curl http://localhost:${LITELLM_PORT}/openapi.json \
  -H "Authorization: Bearer ${MASTER_KEY}" \
  -o openapi.json

# Verify spec
python3 -c "import json; data=json.load(open('openapi.json')); print(f'OpenAPI {data.get(\"openapi\")}, {len(data.get(\"paths\", {}))} endpoints')"
# –û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: OpenAPI 3.x, ~100+ endpoints
```

**–í–∞–∂–Ω–æ:** 
- ‚úÖ –¢—Ä–µ–±—É–µ—Ç—Å—è –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è (Master Key –∏–ª–∏ Virtual Key)
- ‚úÖ –§–æ—Ä–º–∞—Ç: OpenAPI 3.x, JSON
- ‚úÖ –†–∞–∑–º–µ—Ä: ~776KB

### Method 3: Generate from LiteLLM Documentation

LiteLLM follows OpenAI API format. You can use OpenAI's OpenAPI spec as a reference:

- **OpenAI OpenAPI Spec:** https://github.com/openai/openai-openapi
- **LiteLLM Documentation:** https://docs.litellm.ai/

### Using OpenAPI Spec

**Generate API client (Python):**
```bash
# Install openapi-generator
npm install -g @openapitools/openapi-generator-cli

# Generate Python client
openapi-generator-cli generate \
  -i openapi.json \
  -g python \
  -o ./generated-client
```

**Generate API client (TypeScript):**
```bash
# Generate TypeScript client
openapi-generator-cli generate \
  -i openapi.json \
  -g typescript-axios \
  -o ./generated-client
```

**Validate requests:**
```python
from openapi_spec_validator import validate_spec

# Load and validate spec
with open('openapi.json') as f:
    spec = json.load(f)
    validate_spec(spec)
```

## Rate Limits

Rate limits depend on:
- **Provider limits:** Each LLM provider has its own rate limits
- **Resource profile:** Higher profiles allow more concurrent requests
- **Budget profile:** Budget limits may restrict usage

**Check current limits:**
- View in LiteLLM Admin UI: http://localhost:4000/ui
- Check budget settings in `.env`: `BUDGET_PROFILE=test|prod|unlimited`

**Best practices:**
- Use retry logic with exponential backoff
- Implement request queuing for high-volume applications
- Monitor usage in LiteLLM Admin UI
- Use Virtual Keys with specific model/endpoint restrictions

## Best Practices

### 1. Use Virtual Keys
- ‚úÖ Create separate Virtual Keys for different applications
- ‚úÖ Restrict Virtual Keys to specific models/endpoints
- ‚ùå Never use Master Key in client applications

### 2. Error Handling
```python
import requests
from requests.exceptions import RequestException

try:
    response = requests.post(...)
    response.raise_for_status()
except requests.exceptions.HTTPError as e:
    if e.response.status_code == 401:
        print("Authentication failed - check API key")
    elif e.response.status_code == 429:
        print("Rate limit exceeded - retry later")
    else:
        print(f"API error: {e}")
except RequestException as e:
    print(f"Request failed: {e}")
```

### 3. Connection Pooling
```python
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

session = requests.Session()
retry_strategy = Retry(
    total=3,
    backoff_factor=1,
    status_forcelist=[429, 500, 502, 503, 504]
)
adapter = HTTPAdapter(max_retries=retry_strategy)
session.mount("http://", adapter)
session.mount("https://", adapter)
```

### 4. Streaming for Long Responses
```python
import requests

response = requests.post(
    f"{API_BASE}/chat/completions",
    headers={"Authorization": f"Bearer {API_KEY}"},
    json={"model": "claude-sonnet-4-5", "messages": [...], "stream": True},
    stream=True
)

for line in response.iter_lines():
    if line:
        # Parse Server-Sent Events
        if line.startswith(b"data: "):
            data = line[6:]  # Remove "data: " prefix
            if data == b"[DONE]":
                break
            chunk = json.loads(data)
            # Process chunk
```

### 5. Model Selection
- Check available models before making requests
- Use appropriate models for tasks (e.g., Haiku for simple tasks, Opus for complex)
- Consider cost vs. quality trade-offs

## Troubleshooting

### Authentication Errors

**401 Unauthorized:**
- Check API key is correct
- Verify Virtual Key exists in `.env`
- Ensure `Authorization: Bearer` header format is correct

### Connection Errors

**Connection refused:**
- Verify containers are running: `docker compose ps`
- Check port configuration in `.env`
- Ensure firewall allows connections

**Timeout errors:**
- Check LiteLLM logs: `docker compose logs litellm-proxy`
- Verify provider API keys are valid
- Check network connectivity

### Model Not Found

**404 Model not found:**
- Verify model is configured in LiteLLM Admin UI
- Check model ID matches provider's format
- Ensure provider API key is valid

### Rate Limit Errors

**429 Too Many Requests:**
- Implement exponential backoff
- Reduce request frequency
- Check budget limits in `.env`

## Additional Resources

- **LiteLLM Documentation:** https://docs.litellm.ai/
- **OpenAI API Reference:** https://platform.openai.com/docs/api-reference
- **Getting Started Guide:** [Getting Started](../getting-started.md)
- **Configuration Guide:** [Configuration](../configuration.md)
- **Troubleshooting:** [Troubleshooting](../troubleshooting.md)

---

**For questions or issues, check the [Troubleshooting Guide](../troubleshooting.md) or create an issue on GitHub.**

