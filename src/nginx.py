"""
Nginx configuration generation.

See docs/configuration.md#default-configuration-with-nginx---enabled-by-default for nginx configuration.
See docs/nginx/README.md for detailed nginx setup instructions.
"""

from typing import Dict, Any
from .utils import print_success, print_info, ensure_dir


def generate_nginx_config(port_config: Dict[str, Any]) -> None:
    """
    Generate nginx configuration files without relying on envsubst.

    The configs contain native nginx variables (e.g. $scheme) that must stay
    untouched, so we render concrete port numbers directly instead of
    passing the files through envsubst (which would strip those variables).
    
    See docs/nginx/README.md for detailed nginx configuration.
    Note: SSL/HTTPS should be configured via external nginx container.
    """
    if not port_config.get("use_nginx"):
        return
    
    ensure_dir("nginx/conf.d")
    
    litellm_internal_port = port_config.get("litellm_internal_port", 4000)
    webui_internal_port = port_config.get("webui_internal_port", 8080)
    litellm_external_port = port_config.get("litellm_external_port", 4000)
    
    # Generate HTTP only configuration
    # Security: Open WebUI and LiteLLM API exposed on external port via nginx
    # LiteLLM UI and other services accessible only within Docker network or via direct port
    # Note: SSL/HTTPS should be configured via your own nginx container
    config_content = f"""# LiteLLM + Open WebUI Reverse Proxy Configuration
# Auto-generated by setup.py
# HTTP only (no SSL)
# Security: Open WebUI and LiteLLM API exposed via nginx
# Other services (LiteLLM UI) accessible only within Docker network or via direct port

upstream webui_backend {{
    server open-webui:{webui_internal_port};
}}

upstream litellm_backend {{
    server litellm:{litellm_internal_port};
}}

server {{
    listen 80;
    server_name _;
    
    # Client body buffering - optimized for Tier 2 rate limits (RPM: 1,000, ITPM: 500k, OTPM: 50k)
    # Large buffer prevents nginx from writing to temp files for requests up to buffer size
    # This avoids hanging worker processes when reading large request bodies
    # Set to 3M for Tier 2: handles large context requests (200k+ tokens, ~800KB+ JSON payloads)
    # Balanced for memory safety: 3M buffer Ã— 4 workers = ~12MB max per worker in worst case
    # Tier 2 allows significantly higher throughput - buffer sized for memory efficiency
    client_body_buffer_size 3M;
    client_max_body_size 100M;
    client_body_timeout 300s;
    client_header_timeout 300s;

    # Security headers
    add_header X-Frame-Options "SAMEORIGIN" always;
    add_header X-Content-Type-Options "nosniff" always;
    add_header X-XSS-Protection "1; mode=block" always;
    add_header Referrer-Policy "strict-origin-when-cross-origin" always;
    
    # Hide nginx version
    server_tokens off;

    # Health check endpoint (for monitoring)
    location /health {{
        access_log off;
        return 200 "healthy\\n";
        add_header Content-Type text/plain;
    }}

    # Redirect from old path /chat/ to root path /
    location ~ ^/chat(/.*)?$ {{
        return 301 $scheme://$http_host$1;
    }}

    # Health check endpoints - allow for Docker healthcheck and monitoring
    # Healthcheck is used internally by Docker, but also allow external monitoring
    location ~ ^/api/litellm/health(/.*)?$ {{
        rewrite ^/api/litellm/health(.*)$ /health$1 break;
        proxy_pass http://litellm_backend;
        proxy_http_version 1.1;
        access_log off;
        
        # Security: Don't expose internal hostnames
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }}

    # Security: Block LiteLLM UI and admin endpoints
    # Block admin UI and other non-standard endpoints (except health, which is allowed above)
    location ~ ^/api/litellm/(ui|metrics|sse|.*key|.*team|.*user|.*spend|.*config|.*model/new|.*model/delete|.*model/update)(/.*)?$ {{
        return 403 "Access denied: Only standard API endpoints are allowed";
        add_header Content-Type text/plain;
    }}

    # Azure format support
    # Some clients (like continue.dev) automatically use Azure format for certain models
    # Format: /api/litellm/v1/openai/deployments/DEPLOYMENT_NAME/ENDPOINT
    # LiteLLM supports this format and converts it to standard format
    # Supports all standard endpoints: chat/completions, completions, embeddings, models,
    # responses (Azure Responses API for o1, o1-mini, GPT-5), audio/*, images/*, moderations,
    # files/*, fine-tunes/*, assistants/*, threads/*, runs/*, messages/*
    location ~ ^/api/litellm/v1/openai/deployments/([^/]+)/(chat/completions|completions|embeddings|responses|models|audio/|images/|moderations|files/|fine-tunes/|assistants/|threads/|runs/|messages/)(.*)$ {{
        # Extract deployment name and endpoint path, rewrite to standard OpenAI format
        # /api/litellm/v1/openai/deployments/gpt-5-mini/chat/completions -> /v1/chat/completions
        rewrite ^/api/litellm/v1/openai/deployments/[^/]+/(.*)$ /v1/$1 break;
        proxy_pass http://litellm_backend;
        proxy_http_version 1.1;
        
        # Security: Don't expose internal hostnames
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_set_header X-Forwarded-Host $host;
        proxy_set_header X-Forwarded-Port $server_port;
        
        # Streaming support - disable buffering for real-time streaming
        proxy_buffering off;
        proxy_cache off;
        proxy_read_timeout 300s;
        proxy_send_timeout 300s;
        
        # Chunked transfer encoding for streaming
        chunked_transfer_encoding on;
        
        # WebSocket support (if needed for streaming)
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
    }}

    # Support for /responses endpoint (used by some clients like continue.dev)
    location ~ ^/api/litellm/v1/responses(/.*)?$ {{
        rewrite ^/api/litellm/v1/responses(.*)$ /v1/responses$1 break;
        proxy_pass http://litellm_backend;
        proxy_http_version 1.1;
        
        # Security: Don't expose internal hostnames
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_set_header X-Forwarded-Host $host;
        proxy_set_header X-Forwarded-Port $server_port;
        
        # Use large buffer to avoid temp files and prevent hanging requests
        # Large buffer allows nginx to read request body in memory without writing to temp files
        # Set to 3M for Tier 2: handles large context requests (200k+ tokens, ~800KB+ JSON payloads)
        # Balanced for memory safety while supporting Tier 2 workloads
        client_body_buffer_size 3M;
        client_body_timeout 300s;
        
        # Streaming support - disable buffering for real-time streaming
        proxy_buffering off;
        proxy_cache off;
        proxy_read_timeout 300s;
        proxy_send_timeout 300s;
        proxy_connect_timeout 300s;
        
        # Chunked transfer encoding for streaming
        chunked_transfer_encoding on;
        
        # WebSocket support (if needed for streaming)
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
    }}

    # Anthropic native API endpoint support (for provider: anthropic in continue.dev)
    # When continue.dev uses provider: anthropic, it sends requests to /v1/messages
    # Anthropic API uses POST /v1/messages as the main endpoint
    # LiteLLM supports Anthropic native endpoints via /v1/messages
    # This allows reasoning tags (thinking) to work with Claude models
    # Supports both exact path and with query parameters (e.g., /v1/messages?stream=true)
    location ~ ^/api/litellm/v1/messages(\\?.*)?$ {{
        rewrite ^/api/litellm/v1/messages(.*)$ /v1/messages$1 break;
        proxy_pass http://litellm_backend;
        proxy_http_version 1.1;
        
        # Security: Don't expose internal hostnames
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_set_header X-Forwarded-Host $host;
        proxy_set_header X-Forwarded-Port $server_port;
        
        # Streaming support - disable buffering for real-time streaming
        proxy_buffering off;
        proxy_cache off;
        # Extended timeouts for retry handling (allows 3 retries with 60s delays = ~3 minutes)
        proxy_connect_timeout 300s;
        proxy_read_timeout 300s;
        proxy_send_timeout 300s;
        
        # Chunked transfer encoding for streaming
        chunked_transfer_encoding on;
        
        # WebSocket support (if needed for streaming)
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
    }}
    
    # LiteLLM API endpoints (v1 only)
    # Following standard API format: https://platform.openai.com/docs/api-reference
    # LiteLLM uses /v1/ prefix for all standard endpoints
    # Allowed endpoints:
    # - /v1/chat/completions - Chat completions
    # - /v1/completions - Text completions
    # - /v1/embeddings - Embeddings
    # - /v1/models - List models
    # - /v1/audio/* - Audio endpoints (transcriptions, translations, speech)
    # - /v1/images/* - Image generation
    # - /v1/moderations - Content moderation
    # - /v1/files/* - File operations
    # - /v1/fine-tunes/* - Fine-tuning
    # - /v1/assistants/* - Assistants API
    # - /v1/threads/* - Threads API
    # - /v1/runs/* - Runs API
    # - /v1/messages/* - Messages API (with trailing slash, for OpenAI Messages API)
    # Note: /v1/messages (without trailing slash) is handled by Anthropic location above
    # Match /api/litellm/v1/* and proxy to /v1/* on backend
    location ~ ^/api/litellm/v1/(chat/completions|completions|embeddings|models|audio/|images/|moderations|files/|fine-tunes/|assistants/|threads/|runs/|messages/)(.*)$ {{
        rewrite ^/api/litellm/v1/(.*)$ /v1/$1 break;
        proxy_pass http://litellm_backend;
        proxy_http_version 1.1;
        
        # Security: Don't expose internal hostnames
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_set_header X-Forwarded-Host $host;
        proxy_set_header X-Forwarded-Port $server_port;
        
        # Streaming support - disable buffering for real-time streaming
        proxy_buffering off;
        proxy_cache off;
        # Extended timeouts for retry handling (allows 3 retries with 60s delays = ~3 minutes)
        proxy_connect_timeout 300s;
        proxy_read_timeout 300s;
        proxy_send_timeout 300s;
        
        # Chunked transfer encoding for streaming
        chunked_transfer_encoding on;
        
        # WebSocket support (if needed for streaming)
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
    }}
    
    # Block all other /api/litellm/ paths (security)
    location /api/litellm/ {{
        return 403 "Access denied: Only standard API endpoints are allowed. Use /v1/chat/completions, /v1/models, etc.";
        add_header Content-Type text/plain;
    }}

    # Open WebUI - root path (must be last)
    location / {{
        proxy_pass http://webui_backend;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_set_header X-Forwarded-Host $host;
        proxy_set_header X-Forwarded-Port $server_port;
        
        # WebSocket support
        proxy_read_timeout 86400;
        proxy_send_timeout 86400;
    }}
}}
"""
    
    # Write main vhost configuration file
    main_conf_path = "nginx/conf.d/litellm.conf"
    with open(main_conf_path, "w", encoding="utf-8") as f:
        f.write(config_content)
    
    # Note: LiteLLM UI is accessible via direct Docker port mapping (not through nginx)
    # This provides better security - UI only accessible from local/VPN network
    # Port mapping is configured in docker-compose.override.yml
    
    print_success("Nginx configuration created")
    print_success(f"Main vhost: Open WebUI + LiteLLM API at /api/litellm/")
    if litellm_external_port:
        print_success(f"LiteLLM UI: Direct access via Docker port {litellm_external_port} (local/VPN network only)")
    print_info("Note: SSL/HTTPS should be configured via your own nginx container")
    print_info("Example configuration available in: docs/nginx/external-nginx-example.conf")
    print_info("See docs/nginx/README.md for detailed setup instructions")

